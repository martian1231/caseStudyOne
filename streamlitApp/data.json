{"input_format_text": "\n    ## Input Data Format should be same as below format.\n    \n    ### For single datapoint:\n    \n    ```\n    [843, 0, '2016-08-01 03:00:00', 8,'Entertainment/public assembly', 5380, 'nan', 1.0, 26.1, 2.0, 20.6,0.0, 1020.5, 200.0, 2.6]\n    ```\n    \n    ### For multiple datapoints:\n    \n    ```\n    [843, 0, '2017-08-01 03:00:00', 8,'Entertainment/public assembly', 5380, 'nan',\n    1.0, 26.1, 2.0, 20.6,0.0, 1020.5, 200.0, 2.6],\n    [789, 2, '2017-1-1 01:00:00', 7, 'Education', 64583, 1923.0, 1.0, -10.0,\n    'nan', -13.5, 'nan', 1026.0, 70.0, 4.6]\n    ```\n    \n    \\* **NOTE:** To compute error metric(or to provide labels), just insert ';'(semi colon) after features then label.\n   \n   eg:\n    ### For single datapoint (passing labels):\n    \n    ```\n    [789, 2, '2017-1-1 01:00:00', 7, 'Education', 64583, 1923.0, 1.0, -10.0,\n    'nan', -13.5, 'nan', 1026.0, 70.0, 4.6];[571.54]\n    ```\n    \n    ### For multiple datapoints(passing labels):\n    \n    ```\n    [843, 0, '2017-08-01 03:00:00', 8,'Entertainment/public assembly', 5380, 'nan',\n    1.0, 26.1, 2.0, 20.6,0.0, 1020.5, 200.0, 2.6],\n    [789, 2, '2017-1-1 01:00:00', 7, 'Education', 64583, 1923.0, 1.0, -10.0,\n    'nan', -13.5, 'nan', 1026.0, 70.0, 4.6];[6.25, 571.54]\n    ",
 "dataset_text": "\n\\* Note: This dataframe is produced after merging three other dataframe, Read further to know more.\n    \n\n### About the dataset\n\n**The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world.**\n\n\n\n### Files\n\n**train.csv**\n\n- `building_id` - Foreign key for the building metadata.\n- `meter` - The meter id code. Read as `{0: electricity, 1: chilledwater, 2: steam, 3: hotwater}`. Not every building has all meter types.\n- `timestamp` - When the measurement was taken\n- `meter_reading` - The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modelling error. \n- UPDATE: as discussed [here](https://www.kaggle.com/c/ashrae-energy-prediction/discussion/119261), the site 0 electric meter readings are in kBTU.\n\n**building_meta.csv**\n\n- `site_id` - Foreign key for the weather files.\n- `building_id` - Foreign key for `training.csv`\n- `primary_use` - Indicator of the primary category of activities for the building based on [EnergyStar property type definitions](https://www.energystar.gov/buildings/facility-owners-and-managers/existing-buildings/use-portfolio-manager/identify-your-property-type)\n- `square_feet` - Gross floor area of the building\n- `year_built` - Year building was opened\n- `floor_count` - Number of floors of the building\n\n**weather_[train/test].csv**\n\nWeather data from a meteorological station as close as possible to the site.\n\n- `site_id`\n- `air_temperature` - Degrees Celsius\n- `cloud_coverage` - Portion of the sky covered in clouds, in [oktas](https://en.wikipedia.org/wiki/Okta)\n- `dew_temperature` - Degrees Celsius\n- `precip_depth_1_hr` - Millimetres\n- `sea_level_pressure` - Millibar/hectopascals\n- `wind_direction` - Compass direction (0-360)\n- `wind_speed` - Meters per second\n\n**test.csv**\n\nThe submission files use row numbers for ID codes in order to save space on the file uploads. `test.csv` has no feature data; it exists so you can get your predictions into the correct order.\n\n- `row_id` - Row id for your submission file\n- `building_id` - Building id code\n- `meter` - The meter id code\n- `timestamp` - Timestamps for the test data period\n\n**sample_submission.csv**\n\nA valid sample submission.\n\n- All floats in the solution file were truncated to four decimal places; we recommend you do the same to save space on your file upload.\n- There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.\n\n## Evaluation Metric\n\nAs we are dealing with regression problem here, we will use something similar to squared error. We will use slightly modification to squared error called **_Root Mean Squared Logarithmic Error._**\n\n![RMSLE](https://i.imgur.com/krukOMD.png)\n\nUsing square root and log we are scaling the values hence making loss function a little more interpretable.\n    ",
 "roadmap_text": "\n    # Problem statement (a regression problem)\n\nThe problem we are trying to solve is one of the competition hosted by ASHRAE on Kaggle platform. \n\nQuestion : **_'How much does it cost to cool a skyscraper in the summer?'_**. \n\nAnswer: **_'A lot! And not just in dollars, but in environmental impact.'_**\n\nSignificant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, **_are the improvements working?_**\n\nTo answer the above question, we are given a dataset by ASHRAE community. We need to develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe\n\n\n\n# Data leak\nData leak/ leaks are the information unintentionally present in the dataset itself or on the internet which reveals information about the target variable and hence making it easy to predict target variable. \n\nSometimes, data leak could be target variable itself, by just using data leaks (ignoring all other variable) we could predict/ classify target variable pretty accurately while other times it could be additional features (features other than target variable itself) which is highly corelated with target variable.\n\n\n\n# Research-Papers/Solutions/Architectures/Kernels\n\n### Simple Exploration Notebook - ASHRAE \n\nSimple EDA on train, weather and building files. Here we get a visual representation of what's happening inside. We get insights like, some buildings located at the same site (location) have zero meter reading for a few consecutive months(an outlier). There is high meter reading in building 1099 (an outlier).\n\n\n\n### Data Leak\n\nThere is data already present on the internet for test sets and hidden sets. Here are few sites that provides data about target variable (meter reading). [Site 0 data leaks](https://www.oeis.ucf.edu/buildings), [Site 1 data leaks](https://platform.carbonculture.net/communities/ucl/30/apps/assets/list/place/) an so on. These sites provides open energy data for many building including buildings we have in our dataset. There are kernels (like [This](https://www.kaggle.com/yamsam/ashrae-leak-data-station)) which provides target variable information about buildings in our dataset. We won't be using data leaks here, But will surely explores how our model performs with and without data leak targets. [This](https://www.kaggle.com/c/ashrae-energy-prediction/discussion/112841#latest-675067) discussion thread provides links and kernels about data leaks.\n\n**Doing research is really important before taking any step towards solving a problem, The problem we are solving might have been already solved. There is a great saying 'seek and you shall find'** like in our case.\n\n\n\n### Handling Missing Values In Weather CSV File\n\nHere I am using [This research paper](https://www.researchgate.net/publication/313867740_A_review_of_missing_values_handling_methods_on_time-series_data) and [This blog post](https://towardsdatascience.com/all-about-missing-data-handling-b94b8b5d2184) to fill the missing values in “weather.csv”, 'building_meta.csv' and meter reading feature.\n\nMissing data occur due to several problems that are known as the “missingness mechanism”.\n\n**Missing completely at random (MCAR):** In MCAT case, The probability of missingness is not related to any observed data and hence there is no relationship between missing data and any other data (including missing data itself).\n\n**Missing at random (MAR):** In MAR case, The probability of missingness is not related to missing data itself but on the other observed data and hence there is a relationship between missing data and other observations.\n\n**Not missing completely at random (NMAR): I**n NMAR case, The probability of missingness is not related to other data but related to itself and hence there is a relationship between missing data itself.\n\n\n\nImpute methods can be divided into two categories, conventional methods and modern methods. Conventional methods include deletion, mean imputation, and hot decking while estimation techniques is a modern method that is specifically picked to handle missing values in time series data.\n\nAs our missing data is continuous in nature, we will be experimenting with few imputation techniques.\n\n* Using non linear regressor (random forest): We will create random forest regressor to train on non-missing values and predict missing values.\n\n![Random forest](https://i.imgur.com/pJDJ2np.png)\n\n* Autoregression (regression on time series data):  As we are dealing with time series data we will create a regression model based on time and try to predict missing values.\n\n![Auto regression](https://emanthedataguy.files.wordpress.com/2018/09/autoregressivemodel_2.png?w=740)\n\n* Using maximum likely hood estimations: We will create GMM(gaussian mixture model) to learn the distribution parameters and use that model to predict missing values.\n\n![GMM](https://www.mathworks.com/help/examples/stats/win64/ClusterDataUsingAGaussianMixtureModelExample_01.png)\n\n\n\n* Imputing using average based on relation of missing columns to other column/s . For instance, We have missing _'year_built_' and *'floor_count'* column in building_meta.csv file, Here we can assume that _'primary\\_use'_ feature of the building could have some relation to *'floor_count'* column and hence could use mean value of data grouped by _'primary\\_use'_ feature.\n\n\n\n### Feature engineering techniques in time series problem\n\nFeature engineering is the core part of machine learning which cannot be overlooked. With Good features, Even simplest of model can go magic. Feature engineering is the process of creating new features by extracting information from existing features, combining multiple existing features like (summing, multiplication, min, max, mean, standard deviation etc).\n\nWhile engineering features for time series data, we have to be careful, because features are dependent on time and hence, engineered feature will also depend on time.\n\nHere are the what we will use to generate new features based on existing one:\n\n#### Time based features\n\nTime based features are features extracted from time (usually timestamp) like day, day of year, weekend, month, hour etc. This features are important to generate other features based on time (eg sale on weekend etc)\n\nHere are list of available features that we could parse from timestamp using pandas.\n\n![feature engineering time series](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/time-features.png)\n\n\n\n#### Lag Features\n\nLag features are the features which includes data from prior timestep. As we are dealing with time series data where output of current timestep depends on the output of previous timestep, It is common to use target variable to engineer new features. Using lag features means shifting the values either up or down n rows. This way, we are exposing information about back in time which would help the machine learning model to predict present value better.\n\n![feature engineering time series](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/fests4-300x186.png)\n\n\n\n#### Rolling Window Features\n\nRolling window uses window (n of adjacent points) of fixed size and side over to compute statistic like mean, median, standard deviation, min, max and so on. This help us gain information about the past time (days, weeks etc) so we can predict future better. Here we use _n_ previous observation to compute statistic for current timestep.\n\n![sliding window features](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/3hotmk.gif)\n\n\n\n#### Expanding Window Features\n\nSame purpose as rolling window features but the difference is how window moves. Here window expands in one direction. Here we take all the values from the past to compute statistic at the current timestep.\n\n![Expanding window](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/12/output_B4KHcT.gif)\n\n\n\n#### Outlier (target variable)\n\nIn this dataset (just like any other dataset), We have outliers. Target variable is meter reading, we have missing meter reading and/or 0 meter reading for any of the 4 type of meters.\n\nThis could be because of few reasons, \n\n1. No consumption: There is not consume of entry at that point of time and hence meter reading is 0. On of the possible reason is when the building is closed.\n2. Power outage: There could be power outage which has no pattern (power can go out anytime for any reason, eg due to bad weather or repairing purposes)\n3. Special day: Days like weekend or holidays could result in zero reading or even high energy consumption (eg in Christmas or new year etc).\n\n\n\n![img](https://i.imgur.com/7SuIDIC.pnghttps://i.imgur.com/7SuIDIC.pnghttps://i.imgur.com/7SuIDIC.png)\n\n<center>Meter reading of all meter types</center>\n\n**Legend:**\n\n- **X axis:** hours elapsed since Jan 1st 2016, for each of the 4 meter types\n- **Y axis:** building_id\n- **Brown:** meter reading available with non-zero value\n- **Light blue:** meter reading available with zero value\n- **White:** missing meter reading\n\nAs you can see, meter 0 (electricity meter) has less missing values (white region). This is because electricity is the basic energy consumption which we use in all seasons.\n\nFor meter reading 1 (chilled water meter), 2 (steam meter) and 3 (hot water meter) we have lot of missing meter reading, this could be because building might not have meter at all. Not every building has all the meters, So it is obvious to have missing values for all these meters.\n\nLight blue colour represents 0 meter reading. We have consecutive zero meter reading for electricity meter for a dozen of building. We will remove them as they might introduce noise in the data. \n\nFor horizontal blue line in any meter reading, We have consecutive 0 meter reading for the same building. We will try to use imputation methods as mentioned above and see if it improves our leader board score.\n\n\n\n# Challenges With The Dataset\n\n* **Huge dataset:** Train data consist of  20216100 rows (20.21 million, 2016) and test data consist of 40+ million data points (2017 and 2018 combined) which is not memory friendly at all. In such cases we must not keep unwanted data in the main memory otherwise memory would explode and hence process will be terminated.\n* **Data leaks:** Target data is already available on the internet (links given in _RESOURCE_ section).\n\n\\* _Note: we have not used data leaks here._\n\n\n\n# First Cut Approach\n\n**Explain in steps about how you want to approach this problem and the initial experiments that you want to do. (MINIMUM 200 words)**\n\nMy first approach will be to get a baseline score to compare other complex ideas and models . My first approach will be a “Just” approach where I will pre-process data just enough to be fed into a model (e.g. light GBM) and get the score metric. This score will help me compare my complex ideas (feature engineering) and model and will guide me towards right direction and decision making. I will be using light GBM (as used by most of the kagglers, at least for this competition) with 2 fold cross validation data.\n\nOther advance modelling ideas are listed below.\n\n\n\n**Divide and conquer:**  We will train model for each meter type and site id (location) individually.  We will be using light GBM with 2 fold cross validation split and will have total of 16 * 2 = 32 models  (16 models (there are total 16 sites, each ML model per site) and 2 cross validation folds). The idea behind training different ML modes for different site is simple, Because weather will be same for all the buildings located at same site, And if there is any power outage at a site, most of the buildings (if not all) located at corresponding site will be effected. So there is a direct relation between site and building and hence, this could be exploited by training different models for different sites.\n\n\n\n**Ensembling:** Here we will use ensembling of ML algorithms like random forest , multi layer perceptron(ANN), Light GBM, cat boost and XGB. We will be using _max voting_ to get prediction (this is one idea) or try meta classifier (stacking) (this is second idea) to see which performs better.\n\n\n\n**Final model and submission will be selected based on leader board score (both public and private)**\n\n    ",
 "ashrae_intro_text": "\nASHRAE (The American Society of Heating, Refrigerating and Air-Conditioning Engineers) is an American professional association seeking to advance heating, ventilation, air conditioning and refrigeration systems design and construction.\n\n### ASHRAE's Mission and Vision\n\n**Mission:**\nTo serve humanity by advancing the arts and sciences of heating, ventilation, air conditioning, refrigeration and their allied fields.\n\n**Vision:**\nA healthy and sustainable built environment for all.\n\n### Now The Question is: \nQ: How much does it cost to cool a skyscraper in the summer?\n\nA: A lot! And not just in dollars, but in environmental impact.\n\n### Competition Description\n\nSignificant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That’s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don’t work with different building types.\nIn this competition, you’ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.\n"}